{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ExBY7w4V62i",
        "outputId": "1f298159-56f1-407d-d750-d4d3271ad8c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/ismailnasri20/driver-drowsiness-dataset-ddd\n",
            "License(s): unknown\n",
            "Downloading driver-drowsiness-dataset-ddd.zip to /content\n",
            "100% 2.57G/2.58G [00:12<00:00, 232MB/s]\n",
            "100% 2.58G/2.58G [00:12<00:00, 214MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d ismailnasri20/driver-drowsiness-dataset-ddd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Path to the downloaded file\n",
        "zip_path = 'driver-drowsiness-dataset-ddd.zip'  # Corrected the missing closing quote\n",
        "extract_path = 'DrowsyImages'\n",
        "\n",
        "# Create the directory for extraction if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "W0Ly0wK_WI7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the paths to the dataset and the target directories\n",
        "dataset_path = 'DrowsyImages/Driver Drowsiness Dataset (DDD)'\n",
        "output_dir = 'DrowsyTrainer'\n",
        "train_dir = os.path.join(output_dir, 'train')\n",
        "val_dir = os.path.join(output_dir, 'validation')\n",
        "test_dir = os.path.join(output_dir, 'test')\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Create subdirectories for each class\n",
        "classes = ['Drowsy', 'Non Drowsy']\n",
        "for cls in classes:\n",
        "    os.makedirs(os.path.join(train_dir, cls), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, cls), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, cls), exist_ok=True)\n",
        "\n",
        "# Function to split dataset\n",
        "def split_dataset(class_name):\n",
        "    class_path = os.path.join(dataset_path, class_name)\n",
        "    images = [os.path.join(class_path, img) for img in os.listdir(class_path)]\n",
        "    train_imgs, temp_imgs = train_test_split(images, test_size=0.4, random_state=42)\n",
        "    val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Move images to train, validation, and test directories\n",
        "    for img in train_imgs:\n",
        "        shutil.copy(img, os.path.join(train_dir, class_name, os.path.basename(img)))\n",
        "    for img in val_imgs:\n",
        "        shutil.copy(img, os.path.join(val_dir, class_name, os.path.basename(img)))\n",
        "    for img in test_imgs:\n",
        "        shutil.copy(img, os.path.join(test_dir, class_name, os.path.basename(img)))\n",
        "\n",
        "# Split each class\n",
        "for cls in classes:\n",
        "    split_dataset(cls)\n",
        "\n",
        "print('Dataset organized into train, validation, and test directories.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl0MUWHmWfip",
        "outputId": "40cac8c2-b845-4312-aecf-110b5e75b848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset organized into train, validation, and test directories.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, concatenate, GlobalAveragePooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Add, Input\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import LeakyReLU"
      ],
      "metadata": {
        "id": "27woWt_1Wmge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2\n",
        ")"
      ],
      "metadata": {
        "id": "wNe-LHsXWtqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    Flatten(),\n",
        "\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "WNIfCIG7WwRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l3E7E-QXmTu",
        "outputId": "c4e5216f-e437-4cb4-a330-abbbed074223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 62, 62, 32)        128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 31, 31, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 29, 29, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 14, 14, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 12, 12, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 4, 4, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 4, 4, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 2, 2, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 980929 (3.74 MB)\n",
            "Trainable params: 979969 (3.74 MB)\n",
            "Non-trainable params: 960 (3.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = data_gen.flow_from_directory(\n",
        "    'DrowsyTrainer/train',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0riyVDsYM_Q",
        "outputId": "a3adb93b-64ac-4ac7-8543-514dddf33250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20061 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = data_gen.flow_from_directory(\n",
        "    'DrowsyTrainer/validation',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBanjaJ0YQsH",
        "outputId": "ead1033c-761f-41bb-8700-eedc85894209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1671 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = data_gen.flow_from_directory(\n",
        "    'DrowsyTrainer/test',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cacuto8ranYF",
        "outputId": "b89f5ad1-722f-40b2-dd76-26b9ff569dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8359 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data, validation_data=val_data, epochs=25, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMwmQgN2Xq6M",
        "outputId": "a7cab812-8725-46a0-fdf3-cca7dd1a4265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "627/627 [==============================] - 87s 138ms/step - loss: 0.1805 - accuracy: 0.9308 - val_loss: 1.8189 - val_accuracy: 0.5075\n",
            "Epoch 2/25\n",
            "627/627 [==============================] - 86s 138ms/step - loss: 0.0618 - accuracy: 0.9797 - val_loss: 0.8889 - val_accuracy: 0.7385\n",
            "Epoch 3/25\n",
            "627/627 [==============================] - 86s 137ms/step - loss: 0.0444 - accuracy: 0.9865 - val_loss: 3.1559 - val_accuracy: 0.5206\n",
            "Epoch 4/25\n",
            "627/627 [==============================] - 87s 138ms/step - loss: 0.0269 - accuracy: 0.9917 - val_loss: 4.1373 - val_accuracy: 0.5332\n",
            "Epoch 5/25\n",
            "627/627 [==============================] - 86s 138ms/step - loss: 0.0293 - accuracy: 0.9918 - val_loss: 2.2315 - val_accuracy: 0.7026\n",
            "Epoch 6/25\n",
            "627/627 [==============================] - 87s 138ms/step - loss: 0.0244 - accuracy: 0.9919 - val_loss: 3.2843 - val_accuracy: 0.6403\n",
            "Epoch 7/25\n",
            "627/627 [==============================] - 85s 136ms/step - loss: 0.0376 - accuracy: 0.9891 - val_loss: 7.0607 - val_accuracy: 0.4740\n",
            "Epoch 8/25\n",
            "627/627 [==============================] - 86s 137ms/step - loss: 0.0181 - accuracy: 0.9949 - val_loss: 2.9009 - val_accuracy: 0.6086\n",
            "Epoch 9/25\n",
            "627/627 [==============================] - 85s 135ms/step - loss: 0.0179 - accuracy: 0.9946 - val_loss: 2.5067 - val_accuracy: 0.6062\n",
            "Epoch 10/25\n",
            "627/627 [==============================] - 86s 137ms/step - loss: 0.0174 - accuracy: 0.9949 - val_loss: 5.6410 - val_accuracy: 0.5404\n",
            "Epoch 11/25\n",
            "627/627 [==============================] - 87s 138ms/step - loss: 0.0161 - accuracy: 0.9951 - val_loss: 2.5614 - val_accuracy: 0.5883\n",
            "Epoch 12/25\n",
            "627/627 [==============================] - 86s 138ms/step - loss: 0.0138 - accuracy: 0.9959 - val_loss: 3.0070 - val_accuracy: 0.7337\n",
            "Epoch 13/25\n",
            "627/627 [==============================] - 85s 135ms/step - loss: 0.0131 - accuracy: 0.9966 - val_loss: 5.0160 - val_accuracy: 0.5751\n",
            "Epoch 14/25\n",
            "627/627 [==============================] - 86s 137ms/step - loss: 0.0170 - accuracy: 0.9953 - val_loss: 4.3226 - val_accuracy: 0.7157\n",
            "Epoch 15/25\n",
            "627/627 [==============================] - 85s 136ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 7.4337 - val_accuracy: 0.5254\n",
            "Epoch 16/25\n",
            "627/627 [==============================] - 86s 137ms/step - loss: 0.0182 - accuracy: 0.9950 - val_loss: 4.2143 - val_accuracy: 0.6373\n",
            "Epoch 17/25\n",
            "627/627 [==============================] - 85s 136ms/step - loss: 0.0147 - accuracy: 0.9953 - val_loss: 4.0858 - val_accuracy: 0.5560\n",
            "Epoch 18/25\n",
            "627/627 [==============================] - 86s 137ms/step - loss: 0.0084 - accuracy: 0.9974 - val_loss: 6.3146 - val_accuracy: 0.5572\n",
            "Epoch 19/25\n",
            "627/627 [==============================] - 86s 137ms/step - loss: 0.0122 - accuracy: 0.9962 - val_loss: 2.4175 - val_accuracy: 0.7768\n",
            "Epoch 20/25\n",
            "627/627 [==============================] - 85s 136ms/step - loss: 0.0068 - accuracy: 0.9981 - val_loss: 4.8980 - val_accuracy: 0.7247\n",
            "Epoch 21/25\n",
            "627/627 [==============================] - 85s 135ms/step - loss: 0.0165 - accuracy: 0.9965 - val_loss: 5.3365 - val_accuracy: 0.5236\n",
            "Epoch 22/25\n",
            "627/627 [==============================] - 85s 136ms/step - loss: 0.0067 - accuracy: 0.9983 - val_loss: 4.7347 - val_accuracy: 0.6517\n",
            "Epoch 23/25\n",
            "627/627 [==============================] - 86s 137ms/step - loss: 0.0041 - accuracy: 0.9984 - val_loss: 7.6046 - val_accuracy: 0.5308\n",
            "Epoch 24/25\n",
            "627/627 [==============================] - 85s 136ms/step - loss: 0.0155 - accuracy: 0.9963 - val_loss: 5.3699 - val_accuracy: 0.4895\n",
            "Epoch 25/25\n",
            "627/627 [==============================] - 86s 137ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 6.5136 - val_accuracy: 0.5506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_data, steps=test_data.samples // test_data.batch_size)\n",
        "print(f\"Test Accuracy: {test_acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI3DdjMWY251",
        "outputId": "aa18b5c9-88bb-4c33-e424-9909cffb1ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "261/261 [==============================] - 30s 117ms/step - loss: 1.2752 - accuracy: 0.9113\n",
            "Test Accuracy: 0.9112787246704102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('drowsiness_detection_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlt7uMxoeAPG",
        "outputId": "fdee2a7d-6879-4943-a494-feb358e33e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('drowsiness_detection_weights.h5')"
      ],
      "metadata": {
        "id": "L1SqkcfFeDrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "y_pred_prob = model.predict(test_data, steps=test_data.samples // test_data.batch_size)\n",
        "y_pred = np.where(y_pred_prob > 0.5, 1, 0)\n",
        "y_true = test_data.classes\n",
        "\n",
        "report = classification_report(y_true[:len(y_pred)], y_pred, target_names=['Non Drowsy', 'Drowsy'])\n",
        "\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OirWOBGIhtqt",
        "outputId": "adcbf26b-8b15-4c9e-c1bb-82fa17fb22df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "261/261 [==============================] - 31s 117ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  Non Drowsy       0.53      0.47      0.50      4470\n",
            "      Drowsy       0.46      0.53      0.49      3882\n",
            "\n",
            "    accuracy                           0.50      8352\n",
            "   macro avg       0.50      0.50      0.50      8352\n",
            "weighted avg       0.50      0.50      0.50      8352\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CEm0Cbm3iG9K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}